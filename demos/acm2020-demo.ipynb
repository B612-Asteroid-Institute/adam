{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The use-case for V1: Creating a mock LSST catalog with ADAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import adam\n",
    "from astropy.coordinates import SkyCoord\n",
    "from astropy.time import Time\n",
    "from lsst.opsim import Survey, lsstFoV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The general logic I'd propose is that we have (likely immutable) datasets -- such as catalogs and ephemerides -- and transformations on those datasets -- such as propagation, ephemerides computation, or even upload. Importantly, transformations create _new_ datasets, they don't modify existing ones in place. I.e., the patters is always:\n",
    "\n",
    "```python\n",
    "    new_dataset = adam.some_transform(old_dataset, other_args)\n",
    "```\n",
    "\n",
    "This pattern (coming from functional languages) makes it easier to reason about and build scalable, robust, distributed workflows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider everything below pseudocode & pardon my typos and outright mistakes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verson 1: Workflow-agnostic variant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This API concept is similar to what we have today; it hides the details of the server-side workflow behind our custom `adam` API."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First: Load the orbit catalog from a local file and upload it to ADAM. `mpcorb` below is an opaque handle to this dataset (e.g., could be an S3 URL internally) to the uploaded catalog. It is this handle that is passed around when working with other ADAM functions (e.g., propagation or ephemerides computation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orbits = adam.import_orbits_from_file(\"mpcorb.dat\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the LSST survey definition -- this Survey object has member array variables that list the times and directions of all LSST survey pointings (about 3 million over 10 years). Extract the days (in Modified Julian Dates) on which there are any observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lsst = Survey.from_txt('../../baseline.csv.gz')\n",
    "mjd_raw = np.unique(np.floor(lsst.epochs.mjd))\n",
    "nights = Time(mjd_raw, format='mjd', scale=lsst.epoch.scale) # convert to proper astropy.Time objects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the on-sky locations of asteroids in all the nights when LSST observed (one per night). This is the place for ADAM to shine, as we're asking it to propagate ~10M asteroids to ~3000 nights (which will result in ~30Bn rows of ephemerides).\n",
    "\n",
    "Note that the input here are `orbits` (a handle to the uploaded dataset), and the output are `ephems_grid` (a handle to the newly generated ephemerides dataset). Nothing gets downloaded to the client at this point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "ephems_grid = adam.ephemerides(orbits, time=nights, obscode=\"I11\", output_state_vectors=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we take the the generated ephemerides, and for each night look for objects that are inside of LSST observations done within that night.\n",
    "\n",
    "We do that by asking adam to run our user-defined function -- `test_in_FoV` -- on chunks of the (large, 30Bn-row) ephemerides dataset -- a [`map` operation](https://en.wikipedia.org/wiki/Map_(higher-order_function)). This function is [cloudpickled](https://github.com/cloudpipe/cloudpickle) over to server side, and executed on chunks which are passed to it as Pandas DataFrames. It returns chunks that (when all concatenated) form the output dataset.\n",
    "\n",
    "Note: I assume that chunking is arbitrary (i.e., it's not guaranteed that the ADAM runtime will chunk the dataset on per-night (or any other) basis; this could be made more intelligent later (e.g., `groupby` type functionality)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test_in_FoV(ephem_chunk, lsst, approximate):\n",
    "    # Given the (ra, dec, time) for a set of asteroids, the function below returns\n",
    "    # a table of candidate visits that may have observed those asteroids. The table has the\n",
    "    # following columns: (visit_id, visit_time, asteroid_index)\n",
    "\n",
    "    c_visits = lsst.is_observed(ephem_chunk['ra'], ephem_chunk['dec'], ephem_chunk['time'], approximate=approximate)\n",
    "\n",
    "    # now we'll add the asteroid information to each row, so we can use it later\n",
    "    # see https://pandas.pydata.org/pandas-docs/stable/user_guide/merging.html#set-logic-on-the-other-axes for\n",
    "    # what the third line does.\n",
    "    ephem_states = ephem_chunk[\"desig x y z vx vy vz epoch\".split()]\n",
    "    ast_data = ephem_states.iloc[c_visits['asteroid_index']] # this extracts the state vector -- we'll use it later to compute the exact positions\n",
    "    c_visits = pd.concat([c_visits, ast_data], axis=1)\n",
    "    c_visits[\"time\"] = c_visit[\"visit_time\"] # make the \"time\" column correspond to visit time -- this will be used by adam.ephemerides to compute exact positions\n",
    "\n",
    "    return c_visits\n",
    "\n",
    "candidates = adam.map(ephem_grid, lambda chunk: test_in_FoV(chunk, lsst, approximate=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like before, the `candidate_visits` object above is a handle to the server-side dataset. This dataset should now have some ~2Bn rows (we know this from LSST simulations).\n",
    "\n",
    "These are objects that are _near_ LSST observations, but not guaranted to have been observed. So the next step is to compute their exact positions at individual visit times, and check if they were within the field of view. We do that by calling `adam.ephemerides`, passing it the `candidate_visits` dataset. Each row of this dataset has the state vector for the asteroid (x, y, z, vx, vy, vz, epoch) and the `time` for which the ephemeris is desired. `adam.ephemerides` reads this and returns the ephemeris computed at that time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ephems_exact = adam.ephemerides(candidates, obscode=\"I11\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now filter the catalog by running `adam.map` again, this time with `approximate=False` for `test_in_FoV`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mock_catalog = adam.map(ephems, lambda chunk: test_in_FoV(chunk, lsst, approximate=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the generated catalog as a dataframe. This is the first time we're downloading something sizable back. This dataset should be ~1.6Bn rows in size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = mock_catalog.toDF()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verson 2: If using Spark for a workflow manager"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This experiment exposes the underlying workflow manager, and uses it completely to do the parallelization. In this variant, the `adam` Python library are just thin wrappers around integrators plus various utility functions. Below is what the user experience may look like."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we were using Spark to manage the workflow, the most natural (persistent) dataset storage format becomes parquet. The temporary datasets become Spark's Dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import adam\n",
    "from astropy.coordinates import SkyCoord\n",
    "from astropy.time import Time\n",
    "from lsst.opsim import Survey, lsstFoV\n",
    "import lsst.astutils\n",
    "\n",
    "import databricks.koalas as ks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mpcorb = lsst.astutils.loadMPCORB(\"mpcorb.dat\")\n",
    "orbits = ks.from_pandas(mpcorb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lsst = Survey.from_txt('../../baseline.csv.gz')\n",
    "mjd_raw = np.unique(np.floor(lsst.epochs.mjd))\n",
    "nights = Time(mjd_raw, format='mjd', scale=lsst.epoch.scale) # convert to proper astropy.Time objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_mock_catalog(orbits, lsst, nights):\n",
    "    # this is essentially the same code as before, but the key difference is that these are now\n",
    "    # computations that are performed _on the machine_ where this function is running.\n",
    "    # Spark schedules `compute_mock_catalog` to run on a specific node, and then it runs locally\n",
    "    # w/o calling to remote APIs. E.g., adam.ephemerides would probably directly invoke \n",
    "    # pyorb or STK, etc., and the returned value is an actual Pandas dataframe.\n",
    "    #\n",
    "    # So it's _very_ different compared to \"Version 1\" computing model (potentially simpler in\n",
    "    # some ways, and more complex in others.)\n",
    "    #\n",
    "\n",
    "    # approximate ephemerides on a grid, for this subset of orbits\n",
    "    ephems_grid = adam.ephemerides(orbits, time=nights, obscode=\"I11\", output_state_vectors=True)\n",
    "\n",
    "    # figure out who's likely to have been observed\n",
    "    c_visits = lsst.is_observed(ephem_chunk['ra'], ephem_chunk['dec'], ephem_chunk['time'], approximate=True)\n",
    "\n",
    "    # compute the exact locations, at the time of the visit, of the potentially observed asteroids\n",
    "    states = ephems_grid[\"desig x y z vx vy vz epoch\".split()].iloc[c_visits[\"asteroid_index\"]]\n",
    "    ephems_exact = adam.ephemerides(states, time=c_visits[\"visit_time\"], time_per_row=True, obscode=\"I11\")\n",
    "\n",
    "    # intersect with observations\n",
    "    mock_catalog = lsst.is_observed(ephems_exact['ra'], ephems_exact['dec'], ephems_exact['time'])\n",
    "\n",
    "    return mock_catalog\n",
    "\n",
    "result = (\n",
    "    orbits\n",
    "        .groupby(orbits[\"rowid\"] % 10000)            # a hack parallelize over 10000 chunks (there's a better way to do this)\n",
    "        .apply(compute_mock_catalog, lsst, nights)   # for each chunk, do full computation within your function\n",
    "        .toDF()                                      # download the resulting DF\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above has the benefit that no large intermediate datasets are ever created, as the entire pipeline executes in the same function. The flexibility is significantly greater, as full-blown map-reduce workflows can be implemented. It could run natively on Google's Dataproc. Would an Apache Beam implementation looks similar?\n",
    "\n",
    "N.b., we could run the above with Cloud Functions as well (given the pleasing parallelism)..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
